
"""
================================================================================
PHASE 17: ADVANCED FAITHFULNESS & COHERENCE EVALUATION
================================================================================

This script evaluates generated summaries using state-of-the-art metrics:
1. SummaC (NLI-based consistency/hallucination detection)
2. UniEval (T5-based multi-dimensional evaluation: Coherence, Consistency, Fluency)

Target: 1,000 samples across Flat 1024, Flat Overlap, and TreeSum.

Author: Arnav Gupta
Date: 2026-02-08
================================================================================
"""

import os
import sys
import json
import torch
import numpy as np
import pandas as pd
from tqdm import tqdm
import subprocess
from typing import List, Dict, Optional

# ==========================================
# 1. SETUP & DEPENDENCIES
# ==========================================
def setup_eval_env():
    """Install required libraries for SummaC and UniEval."""
    packages = ["summac", "transformers", "torch", "nltk", "pandas", "tqdm"]
    for p in packages:
        try:
            __import__(p)
        except ImportError:
            print(f"Installing {p}...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", p])
    
    import nltk
    nltk.download('punkt', quiet=True)

setup_eval_env()

from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, T5ForConditionalGeneration
from summac.model_summac import SummaCConv

# ==========================================
# 2. CONFIGURATION
# ==========================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# These JSONs are generated by consolidate_summaries.py
CONSOLIDATED_PATHS = {
    "flat_1024": os.path.join(BASE_DIR, "consolidated_flat_1024.json"),
    "flat_overlap": os.path.join(BASE_DIR, "consolidated_flat_overlap.json"),
    "treesum": os.path.join(BASE_DIR, "consolidated_treesum.json")
}

DEVICE = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
print(f"Using device: {DEVICE}")

# ==========================================
# 3. METRIC IMPLEMENTATIONS
# ==========================================

class AdvancedEvaluator:
    def __init__(self, use_unieval=True, use_summac=True):
        self.use_unieval = use_unieval
        self.use_summac = use_summac
        
        if use_summac:
            print("Loading SummaC model...")
            self.summac_model = SummaCConv(models=["vitaminc"], bins='percentile', granularity="sentence", device=DEVICE)
            
        if use_unieval:
            print("Loading UniEval model (MingZhong/UniEval-summarization)...")
            self.unieval_tokenizer = AutoTokenizer.from_pretrained("MingZhong/UniEval-summarization")
            self.unieval_model = T5ForConditionalGeneration.from_pretrained("MingZhong/UniEval-summarization").to(DEVICE)

    def score_summac(self, source: str, summary: str) -> float:
        """Compute SummaC consistency score."""
        if not summary.strip(): return 0.0
        res = self.summac_model.score([source], [summary])
        return float(res["scores"][0])

    def score_unieval(self, source: str, summary: str) -> Dict[str, float]:
        """
        Compute UniEval scores using logit-based probability of 'Yes' vs 'No'.
        """
        dimensions = ["coherence", "consistency", "fluency", "relevance"]
        scores = {}
        
        # Token IDs for 'Yes' (4273) and 'No' (150) in T5-based UniEval
        pos_id, neg_id = 4273, 150
        
        for dim in dimensions:
            prompt = f"question: {dim} content: {source[:2000]} summary: {summary}"
            inputs = self.unieval_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(DEVICE)
            
            with torch.no_grad():
                outputs = self.unieval_model(**inputs, decoder_input_ids=torch.tensor([[0]]).to(DEVICE))
                logits = outputs.logits[0, 0, :]
                probs = torch.softmax(torch.tensor([logits[pos_id], logits[neg_id]]), dim=0)
                scores[dim] = float(probs[0]) # Normalized score (0 to 1)
                
        return scores

# ==========================================
# 4. EXECUTION
# ==========================================

def run_evaluation():
    evaluator = AdvancedEvaluator(use_unieval=False, use_summac=True) # SummaC is the focus for Faithfulness
    
    for method, path in CONSOLIDATED_PATHS.items():
        if not os.path.exists(path):
            print(f"Skipping {method} (File not found yet: {path})")
            continue
            
        print(f"\nEvaluating {method}...")
        with open(path, 'r') as f:
            data = json.load(f)
            
        results = []
        start_time = time.time()
        
        for item in tqdm(data[:50], desc=f"Benchmarking {method}"): # Benchmark first 50 samples
            sample_start = time.time()
            source = item.get("document", "")
            summary = item.get("generated_summary", "")
            
            s_score = evaluator.score_summac(source, summary)
            sample_elapsed = time.time() - sample_start
            
            results.append({
                "sample_id": item["sample_id"],
                "summac_score": s_score,
                "time_sec": sample_elapsed
            })
            
        total_elapsed = time.time() - start_time
        avg_time = total_elapsed / len(results)
        
        # Performance Summary
        print(f"\n{'='*40}")
        print(f"PERFORMANCE REPORT: {method.upper()}")
        print(f"{'='*40}")
        print(f"Device:           {DEVICE}")
        print(f"Total Samples:    {len(results)}")
        print(f"Total Time:       {total_elapsed:.2f}s")
        print(f"Avg Time/Sample:  {avg_time:.4f}s")
        
        # Hardware Comparison (Estimation)
        a40_est = avg_time / 4.0 # A40 is roughly 4x faster for these NLI/T5 tasks
        print(f"Estimated A40 Time: ~{a40_est:.4f}s per sample")
        print(f"{'='*40}")
        
        # Save scores
        out_path = os.path.join(BASE_DIR, f"advanced_metrics_{method}.csv")
        df = pd.DataFrame(results)
        df.to_csv(out_path, index=False)
        print(f"âœ“ Saved scores to {out_path}")

if __name__ == "__main__":
    run_evaluation()
